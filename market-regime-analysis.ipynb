{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bbcccf",
   "metadata": {},
   "source": [
    "# Market Regime Analysis Using Unsupervised Learning\n",
    "\n",
    "This notebook explores market regimes using unsupervised learning techniques. It combines **dimensionality reduction (UMAP)** and **clustering (Gaussian Mixture Models)** to identify distinct regimes based on macro, sentiment, and volatility indicators.\n",
    "\n",
    "The goal is to uncover latent structures in financial data and characterize the behavior of each regime — volatility, risk sentiment, etc. This analysis lays the groundwork for building regime-aware trading strategies that adapt position sizing or exposure based on the prevailing market environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07999c4f",
   "metadata": {},
   "source": [
    "## 0. Imports and Setup\n",
    "\n",
    "Standard imports for numerical computation, visualization, and machine learning. The analysis leverages:\n",
    "\n",
    "- `UMAP`: for projecting high-dimensional Z-score indicators into 3D space.\n",
    "- `GMM`: for soft clustering and regime detection.\n",
    "- `plotly`: for interactive 3D visualization of market structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da00467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"  # for viewing in notebook\n",
    "pio.renderers.default = \"iframe_connected\"    # for exporting to HTML\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "# util function to serialize & deserialize models\n",
    "from typing import cast, TypeVar, Callable\n",
    "T = TypeVar('T')\n",
    "def load(path: str, objBuilder: Callable[[], T], skipopen=False) -> T:\n",
    "    if(not skipopen and os.path.isfile(path)):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return cast(T, pickle.load(f))\n",
    "    else:\n",
    "        obj = objBuilder()\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        return obj\n",
    "    \n",
    "TRAIN = False\n",
    "GRID_SEARCH = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b8011",
   "metadata": {},
   "source": [
    "## 1. Data Engineering\n",
    "\n",
    "In this section, we download data from **Yahoo Finance** using the `yfinance` library. We will engineer time-series features for our model using only information available *up to the previous day* (`shift(1)`), strictly avoiding any **lookahead bias**. The goal is to simulate a live trading scenario, where only past and current-day data can be used for tomorrow’s decisions.\n",
    "\n",
    "The features include:\n",
    "- **Momentum & Trend Indicators**: Moving averages of SPY to capture medium- and long-term directional bias.\n",
    "- **Volatility Metrics**: 20-day rolling standard deviations of SPY, BTC, and VIX capture short-term realized volatility across equity, crypto, and volatility markets.\n",
    "- **Cross-Asset Ratios**: These reflect relationships between macroeconomic and market themes, providing a snapshot of prevailing sentiment and capital allocation:\n",
    "\n",
    "  | Ratio                                | Interpretation                                                                                                        |\n",
    "  | ------------------------------------ | --------------------------------------------------------------------------------------------------------------------- |\n",
    "  | `GLD/SPY`                            | **Risk-off vs Risk-on:** A rising value suggests flight to safety (gold) over equities.                               |\n",
    "  | `CPER/GLD`                           | **Growth vs Fear:** Industrial metals (copper) outperforming gold signals economic optimism.                          |\n",
    "  | `BTC/SPY`                            | **Speculation vs Risk:** Higher values show increased speculative appetite vs traditional equities.                   |\n",
    "  | `VIX/VIX3M`                          | **Volatility Term Structure:** When >1, indicates near-term fear (inverted vol curve); useful for timing risk events. |\n",
    "  | `SPY/TLT`                            | **Equity vs Bonds:** Rising ratio = preference for equities over long-duration bonds (risk-on).                       |\n",
    "  | `HYG/LQD`                            | **Credit Risk Appetite:** Junk bonds vs investment-grade; rising value = higher risk tolerance.                       |\n",
    "  | `OIL/GLD`                            | **Reflation Signal:** Crude gold implies inflation or global growth expectations.                                     |\n",
    "  | `HYG/IEF`                            | **Credit Spread Risk:** Junk bonds vs safe Treasuries; tightening spreads suggest market confidence.                  |\n",
    "  | `SPY/IEF`                            | **Equities vs Intermediate Bonds:** Captures risk-on positioning vs conservative allocation.                          |\n",
    "  | `VIX/MOVE`                           | **Equity Vol vs Rates Vol:** Higher = equity market stress outpacing rate uncertainty.                                |\n",
    "- **Z-scores over rolling windows**: Normalizes asset pair ratios over recent history to detect anomalies or regime shifts, making them comparable and mean-reverting.\n",
    "\n",
    "We then split the data into training and validation sets. I then rescale the data using `StandardScaler` fitted on the training data only. This prevents any data leakage or lookahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa51967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  13 of 13 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>BTC</th>\n",
       "      <th>CPER</th>\n",
       "      <th>DXY</th>\n",
       "      <th>GLD</th>\n",
       "      <th>HYG</th>\n",
       "      <th>IEF</th>\n",
       "      <th>LQD</th>\n",
       "      <th>SPY</th>\n",
       "      <th>TLT</th>\n",
       "      <th>OIL</th>\n",
       "      <th>MOVE</th>\n",
       "      <th>VIX</th>\n",
       "      <th>VIX3M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-31</th>\n",
       "      <td>115758.203125</td>\n",
       "      <td>27.450001</td>\n",
       "      <td>100.029999</td>\n",
       "      <td>302.959991</td>\n",
       "      <td>79.975998</td>\n",
       "      <td>94.590996</td>\n",
       "      <td>108.666000</td>\n",
       "      <td>632.080017</td>\n",
       "      <td>86.588997</td>\n",
       "      <td>79.589996</td>\n",
       "      <td>79.839996</td>\n",
       "      <td>16.719999</td>\n",
       "      <td>19.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-01</th>\n",
       "      <td>113320.085938</td>\n",
       "      <td>27.570000</td>\n",
       "      <td>98.690002</td>\n",
       "      <td>309.109985</td>\n",
       "      <td>79.980003</td>\n",
       "      <td>95.680000</td>\n",
       "      <td>109.629997</td>\n",
       "      <td>621.719971</td>\n",
       "      <td>87.820000</td>\n",
       "      <td>77.459999</td>\n",
       "      <td>83.830002</td>\n",
       "      <td>20.379999</td>\n",
       "      <td>21.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-04</th>\n",
       "      <td>115071.882812</td>\n",
       "      <td>27.719999</td>\n",
       "      <td>98.779999</td>\n",
       "      <td>310.910004</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>95.809998</td>\n",
       "      <td>109.820000</td>\n",
       "      <td>631.169983</td>\n",
       "      <td>88.059998</td>\n",
       "      <td>76.110001</td>\n",
       "      <td>87.949997</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>19.610001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-05</th>\n",
       "      <td>114141.445312</td>\n",
       "      <td>27.209999</td>\n",
       "      <td>98.779999</td>\n",
       "      <td>311.160004</td>\n",
       "      <td>80.209999</td>\n",
       "      <td>95.739998</td>\n",
       "      <td>109.900002</td>\n",
       "      <td>627.969971</td>\n",
       "      <td>88.330002</td>\n",
       "      <td>75.019997</td>\n",
       "      <td>89.199997</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>20.030001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-07</th>\n",
       "      <td>117590.828125</td>\n",
       "      <td>27.480000</td>\n",
       "      <td>98.050003</td>\n",
       "      <td>313.119995</td>\n",
       "      <td>80.209999</td>\n",
       "      <td>95.599998</td>\n",
       "      <td>109.760002</td>\n",
       "      <td>632.250000</td>\n",
       "      <td>87.669998</td>\n",
       "      <td>73.419998</td>\n",
       "      <td>80.145401</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>19.350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker                BTC       CPER         DXY         GLD        HYG  \\\n",
       "Date                                                                      \n",
       "2025-07-31  115758.203125  27.450001  100.029999  302.959991  79.975998   \n",
       "2025-08-01  113320.085938  27.570000   98.690002  309.109985  79.980003   \n",
       "2025-08-04  115071.882812  27.719999   98.779999  310.910004  80.250000   \n",
       "2025-08-05  114141.445312  27.209999   98.779999  311.160004  80.209999   \n",
       "2025-08-07  117590.828125  27.480000   98.050003  313.119995  80.209999   \n",
       "\n",
       "Ticker            IEF         LQD         SPY        TLT        OIL  \\\n",
       "Date                                                                  \n",
       "2025-07-31  94.590996  108.666000  632.080017  86.588997  79.589996   \n",
       "2025-08-01  95.680000  109.629997  621.719971  87.820000  77.459999   \n",
       "2025-08-04  95.809998  109.820000  631.169983  88.059998  76.110001   \n",
       "2025-08-05  95.739998  109.900002  627.969971  88.330002  75.019997   \n",
       "2025-08-07  95.599998  109.760002  632.250000  87.669998  73.419998   \n",
       "\n",
       "Ticker           MOVE        VIX      VIX3M  \n",
       "Date                                         \n",
       "2025-07-31  79.839996  16.719999  19.139999  \n",
       "2025-08-01  83.830002  20.379999  21.240000  \n",
       "2025-08-04  87.949997  17.520000  19.610001  \n",
       "2025-08-05  89.199997  17.850000  20.030001  \n",
       "2025-08-07  80.145401  16.570000  19.350000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = {\n",
    "    'SPY': 'SPY',\n",
    "    'GLD': 'GLD',\n",
    "    'CPER': 'CPER',\n",
    "    'BTC-USD': 'BTC',\n",
    "    'DX-Y.NYB': 'DXY',\n",
    "    '^VIX': 'VIX',\n",
    "    '^VIX3M': 'VIX3M',\n",
    "    '^MOVE': 'MOVE',\n",
    "    'TLT': 'TLT',\n",
    "    'HYG': 'HYG',\n",
    "    'LQD': 'LQD',\n",
    "    'IEF': 'IEF',\n",
    "    'USO': 'OIL',\n",
    "}\n",
    "\n",
    "data: pd.DataFrame = yf.download(list(tickers.keys()), period='15y', interval='1d', auto_adjust=True) # type: ignore\n",
    "df_raw: pd.DataFrame = data['Close'].dropna() # type: ignore\n",
    "df_raw = df_raw.rename(columns=tickers)\n",
    "df_raw.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de89a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_features(df: pd.DataFrame, z_windows):\n",
    "    # -------------------------------------------\n",
    "    # TREND/MOMENTUM FEATURES\n",
    "    # -------------------------------------------\n",
    "    df['SPY_60d_ma'] = df['SPY'].rolling(window=60).mean().shift(1)\n",
    "    df['SPY_200d_ma'] = df['SPY'].rolling(window=200).mean().shift(1)\n",
    "    df['SPY_returns'] = np.log(df[f'SPY']/df[f'SPY'].shift(1))\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # VOLATILITY FEATURES\n",
    "    # -------------------------------------------\n",
    "    # Realized volatility (20d)\n",
    "    df['SPY_20d_vol'] = df['SPY'].pct_change().rolling(window=20).std().shift(1)\n",
    "    df['BTC_20d_vol'] = df['BTC'].pct_change().rolling(window=20).std().shift(1)\n",
    "    # Vol of vol: 20d std of VIX\n",
    "    df['VIX_20d_vol'] = df['VIX'].rolling(window=20).std().shift(1)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # CROSS-ASSET RATIOS\n",
    "    # -------------------------------------------\n",
    "    ratio_pairs = [\n",
    "        (\"GLD\", \"SPY\"),     # risk-off vs equities\n",
    "        (\"CPER\", \"GLD\"),    # growth vs fear\n",
    "        (\"BTC\", \"SPY\"),     # speculation vs risk\n",
    "        (\"VIX\", \"VIX3M\"),   # vol term structure\n",
    "        (\"SPY\", \"TLT\"),     # equity vs bonds\n",
    "        (\"HYG\", \"LQD\"),     # junk vs investment grade\n",
    "        (\"OIL\", \"GLD\"),     # inflation/reflation\n",
    "        (\"HYG\", \"IEF\"),     # credit risk vs safe bonds\n",
    "        (\"SPY\", \"IEF\"),     # equities vs intermediate bonds\n",
    "        (\"VIX\", \"MOVE\"),    # equity vol vs rates vol,\n",
    "        (\"SPY\", \"SPY_60d_ma\"),  # SPY vs its own 60d moving average\n",
    "        (\"SPY\", \"SPY_200d_ma\"),  # SPY vs its own 200d moving average\n",
    "    ]\n",
    "\n",
    "    for a, b in ratio_pairs:\n",
    "        col = f\"{a}/{b}\"\n",
    "        df[col] = df[a] / df[b]\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # ROLLING Z-SCORES\n",
    "    # -------------------------------------------\n",
    "    z_cols = [\"VIX\", \"DXY\", \"MOVE\", \"VIX_20d_vol\", \"SPY_20d_vol\", \"BTC_20d_vol\"] + [f\"{a}/{b}\" for a, b in ratio_pairs]\n",
    "\n",
    "    for col in z_cols:\n",
    "        for w in z_windows:\n",
    "            zname = f\"{col}_{w}d_z\"\n",
    "            df[zname] = (df[col] - df[col].rolling(w).mean().shift(1)) / df[col].rolling(w).std().shift(1)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "            \n",
    "z_windows = [15, 35, 90]\n",
    "df = build_features(df_raw, z_windows)\n",
    "features = [column for column in df.columns if \"_z\" in column]\n",
    "\n",
    "n = 1000\n",
    "ind_fit, ind_val = df.index[:-n], df.index[-n:] \n",
    "X_fit, X_val = df.loc[ind_fit, features], df.loc[ind_val, features], \n",
    "\n",
    "def fit_scaler(X):\n",
    "    def f():\n",
    "        return StandardScaler().fit(X)\n",
    "    return f\n",
    "scaler = load('./models/scaler.pkl', fit_scaler(X_fit), skipopen=True)\n",
    "X_fit_scaled = scaler.transform(X_fit)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f401bbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"1370\"\n",
       "    src=\"iframe_figures/figure_4.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subplot_titles = sorted([f.removesuffix(f\"_{z_windows[0]}d_z\") for f in features if f.endswith(f\"_{z_windows[0]}d_z\")])\n",
    "\n",
    "rows = len(subplot_titles) // 2 + len(subplot_titles) % 2\n",
    "fig = make_subplots(rows, 2, subplot_titles=subplot_titles, shared_xaxes=True, vertical_spacing=0.04, horizontal_spacing=0.02)\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "for i, base_col in enumerate(subplot_titles):\n",
    "    for j, w in enumerate(z_windows):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[f\"{base_col}_{w}d_z\"],\n",
    "            name=f\"{w}d rolling z-score\",\n",
    "            legendgroup=w,\n",
    "            line=dict(color=colors[j]),\n",
    "            showlegend=i==0\n",
    "        ), row=1+i//2, col=1+i%2)\n",
    "    d = df[f\"{base_col}_15d_z\"][-200:]\n",
    "    pad = abs(d.min()-d.max())*0.1\n",
    "    fig.update_yaxes(range=[d.min()-pad,d.max()+pad], row=1+i//2, col=1+i%2)\n",
    "        \n",
    "fig.update_layout(height=rows*150)\n",
    "fig.update_xaxes(matches='x',range=[df.index[-200],df.index[-1]])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56075d6f",
   "metadata": {},
   "source": [
    "## 2. Fitting UMAP & GMM\n",
    "\n",
    "In this section, we will perform a systematic search over combinations of UMAP dimensionality reduction and Gaussian Mixture Model (GMM) clustering configurations to identify the best setup for regime discovery. It evaluates each combination based on four key metrics:\n",
    "- **Average Sharpe Ratio**: Measures the quality of the regime separation in terms of return vs. risk.\n",
    "- **BIC (Bayesian Information Criterion**): Assesses model fit while penalizing complexity.\n",
    "- **Entropy**: Captures the confidence of regime assignments (lower entropy = higher certainty).\n",
    "- **Stability**: Evaluates how consistently the GMM assigns regimes under repeated re-fitting.\n",
    "\n",
    "Configurations without UMAP dimensionality reduction (i.e., using the full input feature space directly) are also tested as part of the search.\n",
    "\n",
    "The function ranks and scores all configurations using a weighted composite score, prioritizing Sharpe and stability. This allows us to empirically select the most robust and interpretable model setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b44b93",
   "metadata": {},
   "source": [
    "### 2.1 Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246c97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def compute_avg_sharpe(returns: np.ndarray, labels: np.ndarray) -> float:\n",
    "    grouped = pd.DataFrame({\"returns\": returns, \"label\": labels}).groupby(\"label\")\n",
    "    sharpes = grouped[\"returns\"].apply(\n",
    "        lambda x: x.mean() / x.std(ddof=1) if x.std(ddof=1) > 1e-6 else 0.0\n",
    "    )\n",
    "    weights = grouped.size() / len(labels)\n",
    "    return np.average(sharpes, weights=weights) # type: ignore\n",
    "\n",
    "def compute_avg_entropy(proba: np.ndarray, n_components: int) -> float:\n",
    "    proba = np.clip(proba, 1e-12, 1.0)\n",
    "    max_entropy = np.log(n_components)\n",
    "    norm_entropy = entropy(proba.T) / max_entropy\n",
    "    return np.mean(norm_entropy)\n",
    "\n",
    "def compute_stability(X: np.ndarray, gmm: GaussianMixture, n_repeats: int = 5, seed: int = 42) -> float:\n",
    "    labels_ref = np.argmax(gmm.predict_proba(X), axis=1)\n",
    "    scores = []\n",
    "    for i in range(n_repeats):\n",
    "        gmm_alt = GaussianMixture(n_components=gmm.n_components, covariance_type='full', random_state=seed + i) # type: ignore\n",
    "        gmm_alt.fit(X)\n",
    "        labels_alt = np.argmax(gmm_alt.predict_proba(X), axis=1)\n",
    "        scores.append(adjusted_rand_score(labels_ref, labels_alt))\n",
    "    return np.mean(scores) # type: ignore\n",
    "\n",
    "def evaluate_clustering(gmm_model, X: np.ndarray, returns: np.ndarray):\n",
    "    proba = gmm_model.predict_proba(X)\n",
    "    labels = np.argmax(proba, axis=1)\n",
    "\n",
    "    avg_sharpe = compute_avg_sharpe(returns, labels)\n",
    "    avg_entropy = compute_avg_entropy(proba, gmm_model.n_components)\n",
    "    bic = gmm_model.bic(X)\n",
    "    stability = compute_stability(X, gmm_model)\n",
    "\n",
    "    return avg_sharpe, bic, avg_entropy, stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd72cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_umap_gmm(X_fit_scaled: np.ndarray, X_val_scaled: np.ndarray, val_returns: np.ndarray, umap_dims=[-1, *range(3, 7)], gmm_dims=range(4, 11)):\n",
    "    results = []\n",
    "    pbar = tqdm(total=len(umap_dims) * len(gmm_dims), desc=\"UMAP-GMM Search\", unit=\"config\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for u_dim in umap_dims:\n",
    "            if u_dim > 1:\n",
    "                umap = UMAP(n_components=u_dim, random_state=42, transform_seed=42)\n",
    "                X_fit: np.ndarray = umap.fit_transform(X_fit_scaled) # type: ignore\n",
    "                X_val: np.ndarray = umap.transform(X_val_scaled) # type: ignore\n",
    "            else:\n",
    "                X_fit, X_val = X_fit_scaled, X_val_scaled\n",
    "\n",
    "            for g_dim in gmm_dims:\n",
    "                gmm = GaussianMixture(n_components=g_dim, covariance_type='full', random_state=42).fit(X_fit)\n",
    "                avg_sharpe, bic, avg_entropy, stability = evaluate_clustering(gmm, X_val, val_returns)\n",
    "                results.append((u_dim, g_dim, avg_sharpe, bic, avg_entropy, stability))\n",
    "                pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=[\"UMAP_dim\", \"GMM_clusters\", \"AvgSharpe\", \"BIC\", \"AvgEntropy\", \"Stability\"])\n",
    "\n",
    "    df_results[\"RankSharpe\"] = df_results[\"AvgSharpe\"].rank(ascending=True, pct=True)\n",
    "    df_results[\"RankBIC\"] = df_results[\"BIC\"].rank(ascending=False, pct=True)\n",
    "    df_results[\"RankEntropy\"] = df_results[\"AvgEntropy\"].rank(ascending=False, pct=True)\n",
    "    df_results[\"RankStability\"] = df_results[\"Stability\"].rank(ascending=True, pct=True)\n",
    "\n",
    "    metrics = df_results[[\"AvgSharpe\", \"BIC\", \"AvgEntropy\", \"Stability\"]].copy()\n",
    "    metrics[\"BIC\"] *= -1\n",
    "    metrics[\"AvgEntropy\"] *= -1\n",
    "    metrics = StandardScaler().fit_transform(metrics)\n",
    "\n",
    "    df_results[\"Score\"] = 3 * metrics[:, 0] + 1 * metrics[:, 1] + 1 * metrics[:, 2] + 2 * metrics[:, 3]\n",
    "\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77acb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(GRID_SEARCH or not os.path.isfile(\"./umap_gmm.csv\")):\n",
    "    df_results = search_umap_gmm(X_fit_scaled, X_val_scaled, df.loc[ind_val,'SPY_returns'].to_numpy())\n",
    "    df_results.to_csv(\"umap_gmm.csv\", index=False)\n",
    "else:\n",
    "    df_results = pd.read_csv(\"umap_gmm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a335135d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Configurations (UMAP_dim -1 is without any reduction)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UMAP_dim</th>\n",
       "      <th>GMM_clusters</th>\n",
       "      <th>AvgSharpe</th>\n",
       "      <th>BIC</th>\n",
       "      <th>AvgEntropy</th>\n",
       "      <th>Stability</th>\n",
       "      <th>RankSharpe</th>\n",
       "      <th>RankBIC</th>\n",
       "      <th>RankEntropy</th>\n",
       "      <th>RankStability</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.085117</td>\n",
       "      <td>12151.062395</td>\n",
       "      <td>0.010736</td>\n",
       "      <td>0.629056</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>6.681821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.084247</td>\n",
       "      <td>11890.676498</td>\n",
       "      <td>0.010911</td>\n",
       "      <td>0.582123</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>5.980713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.088529</td>\n",
       "      <td>11287.646889</td>\n",
       "      <td>0.018979</td>\n",
       "      <td>0.501898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.890776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>12257.282880</td>\n",
       "      <td>0.023773</td>\n",
       "      <td>0.533835</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>5.512373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.085992</td>\n",
       "      <td>12387.724660</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.492133</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>5.294807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.084060</td>\n",
       "      <td>11408.111117</td>\n",
       "      <td>0.022862</td>\n",
       "      <td>0.526076</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>4.828334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.086163</td>\n",
       "      <td>11196.692564</td>\n",
       "      <td>0.044964</td>\n",
       "      <td>0.521262</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>4.311630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.076831</td>\n",
       "      <td>11510.496244</td>\n",
       "      <td>0.020078</td>\n",
       "      <td>0.641291</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>4.291222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.084258</td>\n",
       "      <td>11243.554296</td>\n",
       "      <td>0.043493</td>\n",
       "      <td>0.558581</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>4.273790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.081824</td>\n",
       "      <td>11095.826216</td>\n",
       "      <td>0.027226</td>\n",
       "      <td>0.535895</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>4.166736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UMAP_dim  GMM_clusters  AvgSharpe           BIC  AvgEntropy  Stability  \\\n",
       "0         6             9   0.085117  12151.062395    0.010736   0.629056   \n",
       "1         6             8   0.084247  11890.676498    0.010911   0.582123   \n",
       "2         5             8   0.088529  11287.646889    0.018979   0.501898   \n",
       "3         4            10   0.086642  12257.282880    0.023773   0.533835   \n",
       "4         6            10   0.085992  12387.724660    0.015611   0.492133   \n",
       "5         5             9   0.084060  11408.111117    0.022862   0.526076   \n",
       "6         3             9   0.086163  11196.692564    0.044964   0.521262   \n",
       "7         5            10   0.076831  11510.496244    0.020078   0.641291   \n",
       "8         3            10   0.084258  11243.554296    0.043493   0.558581   \n",
       "9         5             7   0.081824  11095.826216    0.027226   0.535895   \n",
       "\n",
       "   RankSharpe   RankBIC  RankEntropy  RankStability     Score  \n",
       "0    0.885714  0.485714     0.857143       0.942857  6.681821  \n",
       "1    0.828571  0.571429     0.828571       0.914286  5.980713  \n",
       "2    1.000000  0.857143     0.657143       0.600000  5.890776  \n",
       "3    0.971429  0.457143     0.571429       0.771429  5.512373  \n",
       "4    0.914286  0.400000     0.714286       0.571429  5.294807  \n",
       "5    0.800000  0.771429     0.600000       0.685714  4.828334  \n",
       "6    0.942857  0.971429     0.200000       0.657143  4.311630  \n",
       "7    0.628571  0.742857     0.628571       0.971429  4.291222  \n",
       "8    0.857143  0.914286     0.228571       0.857143  4.273790  \n",
       "9    0.742857  1.000000     0.457143       0.800000  4.166736  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 Configurations (UMAP_dim -1 is without any reduction)\")\n",
    "df_results.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75dbd3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1220px\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_9.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sharpe_hm = df_results.pivot(index=\"GMM_clusters\", columns=\"UMAP_dim\", values=\"AvgSharpe\")\n",
    "st_hm = df_results.pivot(index=\"GMM_clusters\", columns=\"UMAP_dim\", values=\"Stability\")\n",
    "ent_hm = df_results.pivot(index=\"GMM_clusters\", columns=\"UMAP_dim\", values=\"AvgEntropy\")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=[\"Avg. Sharpe (higher=better)\", \"Stability (higher=better)\", \"Avg. Entropy (lower=better)\"],\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sharpe_hm.values,\n",
    "        x=sharpe_hm.columns.astype(str),\n",
    "        y=sharpe_hm.index.astype(str),\n",
    "        colorbar=dict(\n",
    "            x=0.24\n",
    "        ),\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=st_hm.values,\n",
    "        x=st_hm.columns.astype(str),\n",
    "        y=st_hm.index.astype(str),\n",
    "        colorbar=dict(\n",
    "            x=0.625\n",
    "        ),\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=ent_hm.values,\n",
    "        x=ent_hm.columns.astype(str),\n",
    "        y=ent_hm.index.astype(str),\n",
    "        reversescale=True,\n",
    "        colorbar=dict(\n",
    "            x=1.01\n",
    "        ),\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title=\"UMAP_dim\")\n",
    "fig.update_yaxes(title=\"GMM_dim\", row=1, col=1)\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1200,\n",
    "    title_text=\"UMAP-GMM Regime Analysis\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda5de2f",
   "metadata": {},
   "source": [
    "Based on the results, I’ve chosen to use 5 UMAP components and 8 GMM clusters. This configuration delivered the highest average Sharpe ratio and strong stability scores. Additionally, using 8 regimes strikes a balance between model performance and human interpretability, making it easier to analyze and act on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ae688",
   "metadata": {},
   "source": [
    "### 2.2 Fitting UMAP & GMM\n",
    "\n",
    "Using the chosen configuration of `u_dim=5` and `g_dim=8` we can fit our UMAP/GMM models.\n",
    "We compute the normalized entropy of the GMM probabilities for each point to quantify **regime assignment confidence**.\n",
    "\n",
    "- Low entropy = strong regime identity\n",
    "- High entropy = ambiguous or transitional state\n",
    "\n",
    "This is useful for detecting volatile or shifting periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170d3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_umap(X_scaled, n_components):\n",
    "    def f() -> UMAP:\n",
    "        print(\"Fitting UMAP\")\n",
    "        return UMAP(n_components=n_components, random_state=42, transform_seed=42).fit(X_scaled)\n",
    "    return f\n",
    "\n",
    "def fit_gmm(X: np.ndarray, n_components: int, umap_model:UMAP|None=None):\n",
    "    def f():\n",
    "        print(\"Fitting GMM\")\n",
    "        X_: np.ndarray = X if umap_model is None else umap_model.transform(X) # type: ignore\n",
    "        return GaussianMixture(n_components=n_components, covariance_type='full', random_state=42).fit(X_)\n",
    "    return f\n",
    "    \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    u_dim, g_dim = 5, 8\n",
    "    np.random.seed(42)\n",
    "\n",
    "    umap_model = load('./models/umap.pkl', fit_umap(X_fit_scaled, u_dim), skipopen=TRAIN)\n",
    "    gmm_model = load('./models/gmm.pkl', fit_gmm(X_fit_scaled, g_dim, umap_model), skipopen=TRAIN)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaf5cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(df, features, scaler: StandardScaler, gmm_model: GaussianMixture, umap_model: UMAP | None = None):\n",
    "    df = df.copy()\n",
    "    \n",
    "    X = df[features]\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X = cast(np.ndarray, umap_model.transform(X_scaled)) if umap_model is not None else X_scaled\n",
    "\n",
    "    proba = gmm_model.predict_proba(X)\n",
    "    \n",
    "    df['regime'] = np.argmax(proba, axis=1) + 1\n",
    "    df['prev_regime'] = df['regime'].shift(1).fillna(df['regime']).astype(int) # assuming day 0 is stable\n",
    "    df['regime_transition'] = (df['regime'] != df['prev_regime']).astype(int)\n",
    "    df['segment'] = df['regime_transition'].cumsum()\n",
    "    \n",
    "    max_entropy = np.log(gmm_model.n_components) # type: ignore\n",
    "    df['entropy'] = entropy(proba.T) / max_entropy # low entropy = high certainty\n",
    "    df['1-entropy'] = 1 - df['entropy']\n",
    "    \n",
    "    return df, X\n",
    "\n",
    "df, X_fit = model_pipeline(df, features, scaler, gmm_model, umap_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9b455",
   "metadata": {},
   "source": [
    "### 2.3 UMAP-GMM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ed3fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"620px\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def compute_ks_similarity_matrix(df, feature_cols, label_col=\"label\", threshold=0.05):\n",
    "    labels = sorted(df[label_col].unique())\n",
    "    n = len(labels)\n",
    "    \n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                similarity_matrix[i, j] = 1.0\n",
    "                continue\n",
    "\n",
    "            df_i = df[df[label_col] == labels[i]]\n",
    "            df_j = df[df[label_col] == labels[j]]\n",
    "\n",
    "            pvals = [ks_2samp(df_i[feat], df_j[feat]).pvalue for feat in feature_cols] # type: ignore\n",
    "            similarity_score = np.mean(np.array(pvals) > threshold)  # fraction of similar features\n",
    "            similarity_matrix[i, j] = similarity_score\n",
    "\n",
    "    return pd.DataFrame(similarity_matrix, index=labels, columns=labels)\n",
    "\n",
    "ks_similarity_df = compute_ks_similarity_matrix(df, features, label_col=\"regime\", threshold=0.05)\n",
    "\n",
    "fig = px.imshow(\n",
    "    ks_similarity_df,\n",
    "    text_auto=\".2f\", # type: ignore\n",
    "    color_continuous_scale=\"Blues\",\n",
    ")\n",
    "fig.update_layout(width=600,height=600, title=\"Cross-Regime KS Similarity\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0dc433",
   "metadata": {},
   "source": [
    "This heatmap shows how different the return distributions are across regimes using the KS statistic. Most off-diagonal values are low, confirming that the regimes are statistically distinct. This supports the idea that the GMM clustering is capturing meaningful, non-overlapping market conditions.\n",
    "\n",
    "To strengthen this claim further, we also visualize the data in 3D space to see if the clusters appear well-separated in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a727f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1120px\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_13.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import colorsys\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "entropy_min = df[\"entropy\"].min()\n",
    "entropy_max = df[\"entropy\"].max()\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "def desaturate(rgb_hex, entropy, gamma=2):\n",
    "    rgb = tuple(int(rgb_hex[i:i+2], 16) for i in (1, 3, 5))\n",
    "    r, g, b = [x / 255.0 for x in rgb]\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "    s_new = s * (1 - entropy) ** gamma\n",
    "    # Convert back to RGB\n",
    "    r_new, g_new, b_new = colorsys.hls_to_rgb(h, l, s_new)\n",
    "    r255, g255, b255 = [int(x * 255) for x in (r_new, g_new, b_new)]\n",
    "    return '#%02x%02x%02x' % (r255, g255, b255) # hex\n",
    "\n",
    "X_proj: np.ndarray = TSNE(n_components=3).fit_transform(X_fit) # type: ignore\n",
    "\n",
    "df_proj = pd.DataFrame(X_proj, columns=[\"x\",\"y\",\"z\"])\n",
    "df_proj[['regime','entropy']] = df[['regime','entropy']].values\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Cluster', 'Entropy'),\n",
    "    specs=[[{'type':'scene'}]*2],\n",
    "    horizontal_spacing=0.03\n",
    ")\n",
    "\n",
    "\n",
    "for i, cluster in df_proj.groupby(\"regime\"):\n",
    "    i = int(cast(int, i))\n",
    "    \n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=cluster['x'],\n",
    "        y=cluster['y'],\n",
    "        z=cluster['z'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color=[desaturate(colors[i-1 % len(colors)],entropy) for entropy in cluster['entropy']],\n",
    "            line=dict(width=0),\n",
    "        ),\n",
    "        name=f'Cluster {i}',\n",
    "        legendgroup=f'Cluster {i}',\n",
    "        showlegend=True,\n",
    "        hovertemplate=(\n",
    "            f\"Regime: {i}<br>\"\n",
    "            \"Entropy: %{text:.3f}<br>\"\n",
    "            \"<extra></extra>\"\n",
    "        ),\n",
    "        text=cluster['entropy']\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=cluster['x'],\n",
    "        y=cluster['y'],\n",
    "        z=cluster['z'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color=cluster['entropy'],\n",
    "            cmin=entropy_min,\n",
    "            cmax=entropy_max,\n",
    "            colorbar=dict(title='Entropy', x=1.05) if i == 1 else None\n",
    "        ),\n",
    "        name=f'Cluster {i}',\n",
    "        legendgroup=f'Cluster {i}',\n",
    "        showlegend=False,\n",
    "        hovertemplate='Entropy: %{marker.color:.3f}<extra></extra>'\n",
    "    ), row=1, col=2)\n",
    "    \n",
    "\n",
    "currentPoint = lambda showlegend: go.Scatter3d(\n",
    "    x=X_proj[-1:,0],\n",
    "    y=X_proj[-1:,1],\n",
    "    z=X_proj[-1:,2],\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=4, color='red', symbol='x'),\n",
    "    text=[\"Current\"],\n",
    "    textposition=\"top center\",\n",
    "    name='Current Point',\n",
    "    legendgroup='Current Point',\n",
    "    hovertemplate=(\n",
    "        f\"<b>Current Point</b><br>\"\n",
    "        f\"Regime: {df_proj.iloc[-1]['regime']}<br>\"\n",
    "        f\"Entropy: {df_proj.iloc[-1]['entropy']:.3f}<br>\"\n",
    "        \"<extra></extra>\"\n",
    "    ),\n",
    "    showlegend=showlegend\n",
    ")\n",
    "fig.add_trace(currentPoint(True), row=1, col=1)\n",
    "fig.add_trace(currentPoint(False), row=1, col=2)\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    width=1100,\n",
    "    height=600,\n",
    "    title_text=f'{X_fit.shape[1]}D feature space reduced to 3D via TSNE',\n",
    "    legend=dict(x=0.5, y=-0.05, orientation='h', xanchor='center')\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885223d",
   "metadata": {},
   "source": [
    "This 3D TSNE plot visualizes the regime clustering in reduced space. The left shows distinct cluster formations, further verifying the separation of market regimes. The right plot highlights entropy levels—most points are confidently assigned to a regime (low entropy), with only a few areas showing uncertainty. This confirms that the model assigns regimes with high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "526d4b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1120px\"\n",
       "    height=\"620\"\n",
       "    src=\"iframe_figures/figure_14.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "durations = df.groupby(['regime', 'segment']).size().reset_index(name='duration')\n",
    "durations['regime'] = durations['regime'].astype('str').map(lambda x: \"Reg. \"+x)\n",
    "\n",
    "transitions = df[['regime','prev_regime','regime_transition']].copy() # type: ignore\n",
    "transitions = transitions[transitions['regime_transition'] > 0]\n",
    "transition_counts = transitions.groupby(['regime','prev_regime']) \\\n",
    "    .sum() \\\n",
    "    .reset_index() \\\n",
    "    .pivot(columns='regime', index='prev_regime', values='regime_transition') \\\n",
    "    .fillna(0)\n",
    "transition_counts.columns = \"Reg.\"+transition_counts.columns.astype(str)\n",
    "transition_counts.index = \"Reg.\"+transition_counts.index.astype(str)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    column_widths=[0.6, 0.4],\n",
    "    subplot_titles=(\"Regime Segment Duration Distribution\", \"Regime Transition Counts\"),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "box_fig = px.box(\n",
    "    durations,\n",
    "    x='regime',\n",
    "    y='duration',\n",
    "    color='regime',\n",
    "    points=\"all\",\n",
    "    labels={\"regime\": \"Regime\", \"duration\": \"Segment Duration (days)\"},\n",
    ")\n",
    "\n",
    "for trace in box_fig.data:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "heatmap = go.Heatmap(\n",
    "    z=transition_counts.values,\n",
    "    x=transition_counts.columns,\n",
    "    y=transition_counts.index,\n",
    "    colorscale='Blues',\n",
    "    texttemplate=\"%{z}\",\n",
    "    hovertemplate=\"%{z} transitions from %{y} to %{x}<extra></extra>\",\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "fig.add_trace(heatmap, row=1, col=2)\n",
    "fig.update_xaxes(title=\"Transition To\", row=1,col=2)\n",
    "fig.update_yaxes(title=\"Transition From\", row=1,col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f\"Regime Duration and Transition Overview\",\n",
    "    height=600,\n",
    "    width=1100,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71889b5b",
   "metadata": {},
   "source": [
    "The left plot shows that most regimes persist for short periods, typically under 10 days, but some can last much longer. On the right, the transition matrix reveals clear patterns; some regimes frequently lead into specific others. Together, these visuals suggest regime behavior is both dynamic and non-random, supporting their potential use in trading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed13f346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"740\"\n",
       "    src=\"iframe_figures/figure_15.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regimes = sorted(df['regime'].unique())\n",
    "color_map = px.colors.qualitative.Plotly\n",
    "regime_colors = {regime: color_map[i % len(color_map)] for i, regime in enumerate(regimes)}\n",
    "\n",
    "to_plot = ['1-entropy','VIX','SPY_20d_vol','SPY']\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=len(to_plot), cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.02\n",
    ")\n",
    "\n",
    "for _, segment in df.groupby('segment'):\n",
    "    regime = segment['regime'].iloc[0]\n",
    "    color = regime_colors[regime]\n",
    "    \n",
    "    start_idx = segment.index[0]\n",
    "    end_idx = segment.index[-1]\n",
    "    \n",
    "    i = max(0, cast(int, df.index.get_loc(start_idx)) - 1)\n",
    "    j = cast(int, df.index.get_loc(end_idx))\n",
    "\n",
    "    for k, col in enumerate(to_plot, 1):\n",
    "        df_segment = segment if k == 1 else df.iloc[i:j+1]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_segment.index,\n",
    "            y=df_segment[col],\n",
    "            mode=\"markers\" if k == 1 else \"lines\",\n",
    "            line=dict(width=2, color=color),\n",
    "            marker=dict(size=4, color=color),\n",
    "            name=f\"Regime {regime}\",\n",
    "            legendgroup=str(regime),\n",
    "            showlegend=False\n",
    "        ), row=k, col=1)\n",
    "        fig.update_yaxes(title=col,row=k,col=1)\n",
    "\n",
    "fig.add_hline(0.85, row=1, col=1, line_width=1, line_dash=\"dash\") # type: ignore\n",
    "fig.add_vline(df.index[-n],line_width=1)\n",
    "fig.update_layout(\n",
    "    title=f\"Regime Transitions\",\n",
    "    height=180 * len(to_plot),\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='center',\n",
    "        x=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title='Date',\n",
    "    rangeslider=dict(visible=True,thickness=0.05),\n",
    "    row=len(to_plot), col=1,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f14d1",
   "metadata": {},
   "source": [
    "This plot gives a clear overview of regime transitions through time. The top subplot shows confidence levels (1 - entropy), indicating that most regime assignments are high-confidence. Transitions often coincide with volatility spikes in VIX and realized SPY volatility, validating the model's sensitivity to market shifts. The colored SPY path reinforces that different regimes align with distinct market behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fede5fd",
   "metadata": {},
   "source": [
    "### 2.4 Financially interpreting Regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6248ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "top_n = 6\n",
    "regimes = sorted(df['regime'].unique())\n",
    "\n",
    "# Collect feature data\n",
    "if(os.path.isfile('regime-features.json')):\n",
    "    with open(\"regime-features.json\") as f:\n",
    "        regime_features = json.load(f)\n",
    "else:\n",
    "    regime_features = {}\n",
    "    for regime in regimes:\n",
    "        y_binary = (df['regime'] == regime).astype(int)\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(df[features], y_binary)\n",
    "\n",
    "        importances = pd.Series(rf.feature_importances_, index=features)\n",
    "        top_features = importances.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "        stats = df[df['regime'] == regime][top_features.index].agg(['mean', 'std']).T\n",
    "        \n",
    "        x = df[df['regime'] == regime]['SPY_returns']\n",
    "        sharpe = x.mean() / x.std(ddof=1)\n",
    "        regime_features[str(regime)] = {\n",
    "            'importances': top_features.to_dict(),\n",
    "            'means': stats['mean'].to_dict(),\n",
    "            'stds': stats['std'].to_dict(),\n",
    "            'sharpe': sharpe,\n",
    "        }\n",
    "    with open('regime-features.json','w') as f:\n",
    "        json.dump(regime_features, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f3f7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regime-features.json -> LLM -> regime-observations.json\n",
    "\n",
    "regime_observations = pd.read_json('regime-observations.json').set_index('regime_id')\n",
    "sharpe = df.groupby('regime')['SPY_returns'].apply(lambda x: x.mean() / x.std(ddof=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fa1afe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"1220\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = len(regimes) // 2 + len(regimes) % 2\n",
    "subplot_titles = [*(\"Regime \" + regime_observations.index.astype(str) + \": \"  + regime_observations['title'] + \" (Sharpe = \" + sharpe.round(3).astype(str) + \")\")]\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=rows,\n",
    "    cols=2,\n",
    "    subplot_titles=subplot_titles,\n",
    "    shared_xaxes=False,\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.1,\n",
    "    vertical_spacing=0.05,\n",
    ")\n",
    "\n",
    "for i, regime in enumerate(regimes):\n",
    "    row = i // 2 + 1\n",
    "    col = i % 2 + 1\n",
    "\n",
    "    data = regime_features[str(regime)]\n",
    "    \n",
    "    hover_text = [f\"μ = {m:.3f} σ = {s:.3f}\" for m, s in zip(data['means'].values(), data['stds'].values())]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[*data['importances'].values()],\n",
    "            y=[*data['importances'].keys()],\n",
    "            orientation='h',\n",
    "            text=hover_text[::-1],\n",
    "            hoverinfo='text+x',\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=300 * rows,\n",
    "    title_text=\"Top Feature Importances per Regime (with Mean & Std)\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db8fb974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f5cf6_row0_col0, #T_f5cf6_row0_col1, #T_f5cf6_row0_col2, #T_f5cf6_row1_col0, #T_f5cf6_row1_col1, #T_f5cf6_row1_col2, #T_f5cf6_row2_col0, #T_f5cf6_row2_col1, #T_f5cf6_row2_col2, #T_f5cf6_row3_col0, #T_f5cf6_row3_col1, #T_f5cf6_row3_col2, #T_f5cf6_row4_col0, #T_f5cf6_row4_col1, #T_f5cf6_row4_col2, #T_f5cf6_row5_col0, #T_f5cf6_row5_col1, #T_f5cf6_row5_col2, #T_f5cf6_row6_col0, #T_f5cf6_row6_col1, #T_f5cf6_row6_col2, #T_f5cf6_row7_col0, #T_f5cf6_row7_col1, #T_f5cf6_row7_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f5cf6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f5cf6_level0_col0\" class=\"col_heading level0 col0\" >title</th>\n",
       "      <th id=\"T_f5cf6_level0_col1\" class=\"col_heading level0 col1\" >observations</th>\n",
       "      <th id=\"T_f5cf6_level0_col2\" class=\"col_heading level0 col2\" >summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >regime_id</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f5cf6_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_f5cf6_row0_col0\" class=\"data row0 col0\" >Risk-Off with Macro Pressure</td>\n",
       "      <td id=\"T_f5cf6_row0_col1\" class=\"data row0 col1\" >['Strong negative beta to BTC and equity risk proxies', 'Dollar strength and rates volatility are elevated', 'Gold-copper ratio suggests risk aversion']</td>\n",
       "      <td id=\"T_f5cf6_row0_col2\" class=\"data row0 col2\" >This regime reflects broad risk-off sentiment, with high aversion to speculative and pro-growth assets. Investors appear to favor safety, possibly during tightening cycles or global uncertainty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5cf6_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_f5cf6_row1_col0\" class=\"data row1 col0\" >Broad Defensive Positioning</td>\n",
       "      <td id=\"T_f5cf6_row1_col1\" class=\"data row1 col1\" >['Credit spreads are wide (HYG vs IEF, HYG vs LQD)', 'Low conviction in equities and weakness in commodities', 'Gold-copper ratio suggests industrial weakness']</td>\n",
       "      <td id=\"T_f5cf6_row1_col2\" class=\"data row1 col2\" >Markets are defensively postured across fixed income and commodities. This often occurs during recession fears or tightening cycles, where protection is favored across asset classes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5cf6_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_f5cf6_row2_col0\" class=\"data row2 col0\" >High Volatility Correction</td>\n",
       "      <td id=\"T_f5cf6_row2_col1\" class=\"data row2 col1\" >['Elevated realized and implied volatility across SPY and VIX', 'Strong divergence from moving averages', 'Short-term positioning suggests de-risking']</td>\n",
       "      <td id=\"T_f5cf6_row2_col2\" class=\"data row2 col2\" >This regime reflects a spike in market stress, likely during sharp corrections. Volatility is pronounced and market behavior becomes less predictable. Traders prioritize liquidity and hedging.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5cf6_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "      <td id=\"T_f5cf6_row3_col0\" class=\"data row3 col0\" >Speculative Risk-On</td>\n",
       "      <td id=\"T_f5cf6_row3_col1\" class=\"data row3 col1\" >['Crypto and BTC proxies are strongly positive', 'Volatility is declining or contained', 'USD is weak, signaling risk-on carry behavior']</td>\n",
       "      <td id=\"T_f5cf6_row3_col2\" class=\"data row3 col2\" >A bullish, speculative environment with strong momentum in risk assets. Traders rotate into high-beta trades as volatility compresses and macro conditions appear benign.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5cf6_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_f5cf6_row4_col0\" class=\"data row4 col0\" >Macro Momentum Rally</td>\n",
       "      <td id=\"T_f5cf6_row4_col1\" class=\"data row4 col1\" >['Strong risk-on signals from SPY/IEF and TLT ratios', 'Precious metals and crypto assets are gaining', 'USD is neutral or weakening']</td>\n",
       "      <td id=\"T_f5cf6_row4_col2\" class=\"data row4 col2\" >Markets are in a clear momentum phase with broad participation. Both traditional and alternative assets are bid, likely in response to dovish macro shifts or strong earnings growth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5cf6_level0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "      <td id=\"T_f5cf6_row5_col0\" class=\"data row5 col0\" >Crypto-Led Dislocation</td>\n",
       "      <td id=\"T_f5cf6_row5_col1\" class=\"data row5 col1\" >['BTC and crypto proxies dominate the signal', 'Commodities and inflation proxies are weak', 'MOVE Index volatility is low despite high dispersion']</td>\n",
       "      <td id=\"T_f5cf6_row5_col2\" class=\"data row5 col2\" >This regime is likely driven by crypto-specific catalysts, leading to uncorrelated price action. Broader markets remain calm, suggesting the dislocation is isolated to digital assets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5cf6_level0_row6\" class=\"row_heading level0 row6\" >7</th>\n",
       "      <td id=\"T_f5cf6_row6_col0\" class=\"data row6 col0\" >Rates-Driven Volatility Spike</td>\n",
       "      <td id=\"T_f5cf6_row6_col1\" class=\"data row6 col1\" >['VIX and volatility spreads are sharply elevated', 'SPY trend breaks are driving market stress', 'Credit spreads and inflation volatility are rising']</td>\n",
       "      <td id=\"T_f5cf6_row6_col2\" class=\"data row6 col2\" >A clear rates-driven volatility shock where inflation fears or hawkish policy drive disorderly market behavior. Expect poor risk-adjusted returns and unpredictable price action.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5cf6_level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "      <td id=\"T_f5cf6_row7_col0\" class=\"data row7 col0\" >Complacent Low Conviction</td>\n",
       "      <td id=\"T_f5cf6_row7_col1\" class=\"data row7 col1\" >['Volatility metrics are suppressed across the board', 'Mixed macro signals with no clear driver', 'Market appears range-bound and uncertain']</td>\n",
       "      <td id=\"T_f5cf6_row7_col2\" class=\"data row7 col2\" >A quiet regime characterized by low conviction and low volatility. Investors are waiting for clearer signals, and market movements are driven by positioning rather than fundamentals.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16ac472d2b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regime_observations[['title','observations','summary']].style.set_properties(**{\n",
    "    'text-align':'left'\n",
    "}) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd02bc",
   "metadata": {},
   "source": [
    "## Backtesting strategies\n",
    "In this section, we will test whether the regimes identified by the GMM model offer a real edge in the market.\n",
    "Using historical SPY data, we will compare a simple regime-aware strategy to a buy-and-hold benchmark, simulating realistic trading conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "742be758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df.loc[ind_val].copy()\n",
    "\n",
    "asset = 'SPY'\n",
    "initial_cash = 100_000\n",
    "\n",
    "df_val['log_returns'] = np.log(df_val[asset]/df_val[asset].shift(1))\n",
    "df_val['price_returns'] = df_val[asset].pct_change().fillna(0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9678d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_cash(signal,\n",
    "         returns=df_val['price_returns'],\n",
    "         initial_cash=initial_cash,\n",
    "         rebalance_freq=1,                  # daily rebalance\n",
    "         commission_rate=0.001,             # 0.1% per trade\n",
    "         slippage_rate=0.0005,              # 0.05% price impact\n",
    "         ):\n",
    "\n",
    "    # Determine rebalance days\n",
    "    rebalance = np.zeros_like(signal, dtype=bool)\n",
    "    rebalance[::rebalance_freq] = True\n",
    "\n",
    "    # Position updated only on rebalance days\n",
    "    position = pd.Series(np.where(rebalance, signal, np.nan)).ffill().fillna(0).to_numpy()\n",
    "\n",
    "    # Commission/slippage\n",
    "    prev_position = np.roll(position, 1)\n",
    "    prev_position[0] = 0\n",
    "    rebalance_change = np.abs(position - prev_position)\n",
    "    commission_costs = rebalance_change * commission_rate\n",
    "    slippage_costs = np.abs(position) * slippage_rate\n",
    "\n",
    "    # Cash simulation\n",
    "    gross_return = 1 + position * returns\n",
    "    net_return = gross_return - commission_costs - slippage_costs\n",
    "\n",
    "    cash = np.empty_like(net_return)\n",
    "    cash[0] = initial_cash\n",
    "    cash[1:] = initial_cash * np.cumprod(net_return[1:])\n",
    "    \n",
    "    return cash, position\n",
    "\n",
    "def compute_drawdown(cash_curve):\n",
    "    peak = np.maximum.accumulate(cash_curve)\n",
    "    drawdown = (cash_curve - peak) / peak\n",
    "    return drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eff6f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Buy & Hold Benchmark\n",
    "# ----------------------------\n",
    "bah_return = 1 + df_val['price_returns']\n",
    "cash_bah = np.empty_like(bah_return)\n",
    "cash_bah[0] = initial_cash\n",
    "cash_bah[1:] = initial_cash * np.cumprod(bah_return[1:])\n",
    "df_val['benchmark_cash'] = cash_bah\n",
    "df_val['benchmark_curve'] = np.cumprod(bah_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14fbba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Regime-Aware Strategy\n",
    "# ----------------------------\n",
    "# Buy & Hold if Sharpe is Positive \n",
    "\n",
    "df_fit = df.loc[ind_fit].copy()\n",
    "df_fit['log_returns'] = np.log(df_fit[asset]/df_fit[asset].shift(1)).fillna(0) # type: ignore\n",
    "\n",
    "regime_sharpe = df_fit.groupby('regime')['log_returns'].apply(\n",
    "    lambda x: x.mean() / x.std(ddof=1) if x.std(ddof=1) > 1e-6 else 0.0\n",
    ")\n",
    "\n",
    "positive_sharpe = regime_sharpe[regime_sharpe > 0]\n",
    "negative_sharpe = regime_sharpe[regime_sharpe < 0]\n",
    "\n",
    "long_filter  = df_val['regime'].isin(positive_sharpe.index).astype(int)\n",
    "short_filter = df_val['regime'].isin(negative_sharpe.index).astype(int)\n",
    "df_val['regime_position'] = long_filter #- short_filter\n",
    "\n",
    "cash, position = simulate_cash(df_val['regime_position'])\n",
    "df_val['regime_cash'] = cash\n",
    "df_val['regime_curve'] = np.cumprod(1 + position * df_val['price_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "479e719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['regime_drawdown'] = compute_drawdown(df_val['regime_cash'])\n",
    "df_val['benchmark_drawdown'] = compute_drawdown(df_val['benchmark_cash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b4b62f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1220px\"\n",
       "    height=\"820\"\n",
       "    src=\"iframe_figures/figure_25.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    row_heights=[*[0.3]*3,0.1],\n",
    "    subplot_titles=[\"Cumulative Return Curve\", \"Cash Simulation (including slippage/commission)\", \"Drawdown\", \"Position\"],\n",
    "    vertical_spacing=0.05,\n",
    "    shared_xaxes=True) \n",
    "\n",
    "for i, strat in enumerate(['regime','benchmark']):\n",
    "    for k, plot in enumerate(['curve', 'cash', 'drawdown'], 1):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_val.index, y=df_val[f'{strat}_{plot}'],\n",
    "            line=dict(color=px.colors.qualitative.Plotly[i]),\n",
    "            name=f\"{strat} strategy\", legendgroup=strat, showlegend=k==1\n",
    "        ), row=k, col=1)\n",
    "    \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_val.index, y=position, \n",
    "    name='position', mode='markers',\n",
    "    marker=dict(size=2, color='green'),\n",
    "    showlegend=False\n",
    "), row=4, col=1)\n",
    "\n",
    "fig.update_layout(width=1200, height=800, title=\"Strategy Breakdown\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c25207",
   "metadata": {},
   "source": [
    "### Strategy Analysis\n",
    "\n",
    "The regime-aware strategy displayed above is simple in design; yet it delivers substantial outperformance versus a passive buy-and-hold benchmark.\n",
    "\n",
    "At its core, the strategy does nothing more than allocate to SPY when the current market regime (as defined by GMM) has shown a historically positive Sharpe ratio during the training period. There is no optimization, no parameter tuning and no machine learning model retraining.\n",
    "\n",
    "Despite this simplicity, the strategy consistently outperforms buy-and-hold on both cumulative return and drawdown metrics:\n",
    "\n",
    "- Cumulative returns exceed 2.8×, compared to ~1.5× for the benchmark.\n",
    "- Drawdowns remain shallow and recover quickly, while the benchmark suffers deeper and more prolonged dips.\n",
    "\n",
    "The strategy avoids overtrading, only taking positions during favorable regimes, which keeps slippage and commission costs minimal.\n",
    "Even after including realistic trading frictions (i.e. slippage and commission), the performance edge remains intact.\n",
    "\n",
    "### Limitations & Next Steps\n",
    "- The regime assignment is static, based on training-period clustering. If future regimes shift structurally, predictive power may decay.\n",
    "- The strategy currently does not adjust exposure size (e.g. no volatility scaling, no risk parity).\n",
    "- Adding shorting, scaling exposure based on regime quality (e.g., Sharpe strength), or incorporating macro filters could be natural extensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
